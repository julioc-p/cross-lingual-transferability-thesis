% EXAMPLE ENTRY:
@article{garraway_genomics-driven_2013,
	title = {Genomics-Driven Oncology: Framework for an Emerging Paradigm},
	volume = {31},
	issn = {0732-183X},
	url = {https://ascopubs.org/doi/10.1200/JCO.2012.46.8934},
	doi = {10.1200/JCO.2012.46.8934},
	shorttitle = {Genomics-Driven Oncology},
	abstract = {A majority of cancers are driven by genomic alterations that dysregulate key oncogenic pathways influencing cell growth and survival. However, the ability to harness tumor genetic information for its full clinical potential has only recently become manifest. Over the past several years, the convergence of discovery, technology, and therapeutic development has created an unparalleled opportunity to test the hypothesis that systematic knowledge of genomic information from individual tumors can improve clinical outcomes for many patients with cancer. Rigorous evaluation of this genomics-driven cancer medicine hypothesis will require many logistic innovations that are guided by overarching conceptual advances in tumor genomic profiling, data interpretation, clinical trial design, and the ethical return of genetic results to oncologists and their patients. The results of these efforts and the rigor with which they are implemented will determine whether and how comprehensive tumor genomic information may become incorporated into the routine care of patients with cancer.},
	pages = {1806--1814},
	number = {15},
	journal = {Journal of Clinical Oncology},
	shortjournal = {{JCO}},
	author = {Garraway, Levi A.},
	urldate = {2022-10-31},
	date = {2013-05-20},
	year = {2013},
	note = {Publisher: Wolters Kluwer}
	}

@article{muthukrishnan2020brief,
  title={Brief history of artificial intelligence},
  author={Muthukrishnan, Nikesh and Maleki, Farhad and Ovens, Katie and Reinhold, Caroline and Forghani, Behzad and Forghani, Reza and others},
  journal={Neuroimaging Clinics of North America},
  volume={30},
  number={4},
  pages={393--399},
  year={2020},
  publisher={Elsevier BV}
}

@book{Boole1854-BOOTLO-4,
	address = {London,},
	author = {George Boole},
	editor = {},
	publisher = {The Open court publishing company},
	title = {The Laws of Thought (1854)},
	year = {1854}
}


@incollection{bechtel2007cognitive,
  author    = {William Bechtel},
  title     = {Cognitive Science: History},
  booktitle = {International Encyclopedia of the Social & Behavioral Sciences},
  publisher = {Elsevier Science Ltd.},
  year      = {2007},
  pages     = {2154--2158},
  isbn      = {0-08-043076-7},
  note      = {Copyright 2001 Elsevier Science Ltd. All rights reserved},
  url       = {http://mechanism.ucsd.edu/teaching/w07/philpsych/bechtel.cogscihistory.pdf}
}

@book{turing2009computing,
  title={Computing machinery and intelligence},
  author={Turing, Alan M},
  year={2009},
  publisher={Springer}
}

@article{article,
author = {HODGES, ANDREW},
year = {2006},
month = {09},
pages = {470 - 471},
title = {B. JACK COPELAND (ed.), The Essential Turing: The Ideas that Gave Birth to the Computer Age},
volume = {39},
journal = {The British Journal for the History of Science},
doi = {10.1017/S0007087406448688}
}


@article{howard2019artificial,
  title={Artificial intelligence: Implications for the future of work},
  author={Howard, John},
  journal={American journal of industrial medicine},
  volume={62},
  number={11},
  pages={917--926},
  year={2019},
  publisher={Wiley Online Library}
}

@book{bauer2009,
  author = {Bauer, Friedrich Ludwig},
  title = {Origins and Foundations of Computing: In Cooperation with Heinz Nixdorf MuseumsForum},
  publisher = {Springer Science \& Business Media},
  year = {2009},
  date = {2009-11-05},
  pages = {78},
  isbn = {978-3-64202992-9},
  note = {Retrieved 2022-07-10}
}

@article{doi:10.1177/154193120605000904,
author = {Leo Gugerty},
title ={Newell and Simon's Logic Theorist: Historical Background and Impact on Cognitive Modeling},

journal = {Proceedings of the Human Factors and Ergonomics Society Annual Meeting},
volume = {50},
number = {9},
pages = {880-884},
year = {2006},
doi = {10.1177/154193120605000904},

URL = { 
    
        https://doi.org/10.1177/154193120605000904
    
    

},
eprint = { 
    
        https://doi.org/10.1177/154193120605000904
    
    

}
,
    abstract = { Fifty years ago, Newell and Simon (1956) invented a “thinking machine” called the Logic Theorist. The Logic Theorist was a computer program that could prove theorems in symbolic logic from Whitehead and Russell's Principia Mathematica. This was perhaps the first working program that simulated some aspects of peoples' ability to solve complex problems. The Logic Theorist and other cognitive simulations developed by Newell and Simon in the late 1950s had a large impact on the newly developing field of information-processing (or cognitive) psychology. Many of the novel ideas about mental representation and problem solving instantiated in the Logic Theorist are still a central part of the theory of cognitive psychology, and are still used in modeling the complex tasks studied in human factors psychology. This paper presents some of the theoretical precursors of the Logic Theorist, describes the principles and implementation of the program, and discusses its immediate and long-term impacts. }
}

@article{CAMPBELL200257,
title = {Deep Blue},
journal = {Artificial Intelligence},
volume = {134},
number = {1},
pages = {57-83},
year = {2002},
issn = {0004-3702},
doi = {https://doi.org/10.1016/S0004-3702(01)00129-1},
url = {https://www.sciencedirect.com/science/article/pii/S0004370201001291},
author = {Murray Campbell and A.Joseph Hoane and Feng-hsiung Hsu},
keywords = {Computer chess, Game tree search, Parallel search, Selective search, Search extensions, Evaluation function},
abstract = {Deep Blue is the chess machine that defeated then-reigning World Chess Champion Garry Kasparov in a six-game match in 1997. There were a number of factors that contributed to this success, including: •a single-chip chess search engine,•a massively parallel system with multiple levels of parallelism,•a strong emphasis on search extensions,•a complex evaluation function, and•effective use of a Grandmaster game database. This paper describes the Deep Blue system, and gives some of the rationale that went into the design decisions behind Deep Blue.}
}

@article{10.1145/357980.357991,
author = {Weizenbaum, Joseph},
title = {ELIZA — a computer program for the study of natural language communication between man and machine},
year = {1983},
issue_date = {Jan. 1983},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {26},
number = {1},
issn = {0001-0782},
url = {https://doi.org/10.1145/357980.357991},
doi = {10.1145/357980.357991},
abstract = {ELIZA is a program operating within the MAC time-sharing system of MIT which makes certain kinds of natural language conversation between man and computer possible. Input sentences are analyzed on the basis of decomposition rules which are triggered by key words appearing in the input text. Responses are generated by reassembly rules associated with selected decomposition rules. The fundamental technical problems with which ELIZA is concerned are: (1) the identification of key words, (2) the discovery of minimal context, (3) the choice of appropriate transformations, (4) generation of responses in the absence of key words, and (5) the provision of an editing capability for ELIZA “scripts”. A discussion of some psychological issues relevant to the ELIZA approach as well as of future developments concludes the paper.},
journal = {Commun. ACM},
month = jan,
pages = {23–28},
numpages = {6}
}

@article{ayodele2010types,
  title={Types of machine learning algorithms},
  author={Ayodele, Taiwo Oladipupo},
  journal={New advances in machine learning},
  volume={3},
  number={19-48},
  pages={5--1},
  year={2010}
  }


@article{amini2024self,
  title={Self-training: A survey},
  author={Amini, Massih-Reza and Feofanov, Vasilii and Pauletto, Loic and Hadjadj, Lies and Devijver, Emilie and Maximov, Yury},
  journal={Neurocomputing},
  pages={128904},
  year={2024},
  publisher={Elsevier}
}

@misc{tikz_neural_networks,
  author    = {TikZ.net},
  title     = {Neural Networks},
  year      = {n.d.},
  howpublished = {\url{https://tikz.net/neural_networks/}},
  note      = {Accessed: 2024-12-09}
}

@article{plos_computational_biology_2010,
  title = {PLoS Computational Biology Issue Image},
  author = {},
  journal = {PLoS Computational Biology},
  volume = {6},
  number = {8},
  pages = {ev06.i08},
  year = {2010},
  month = {August},
  doi = {10.1371/image.pcbi.v06.i08},
  url = {https://doi.org/10.1371/image.pcbi.v06.i08}
}


@misc{mit_neural_networks,
  title = {Neural Networks - Basic Element},
  author = {{MIT Open Learning Library}},
  year = {2019},
  note = {Massachusetts Institute of Technology, accessed on December 10, 2024},
  url = {https://openlearninglibrary.mit.edu/},
}

@misc{seer_neurons,
  title = {Neurons and Glial Cells - Brain and CNS Tumors},
  author = {{SEER Training Modules}},
  year = {2024},
  note = {U.S. National Institutes of Health, National Cancer Institute. Accessed on December 10, 2024},
  url = {https://training.seer.cancer.gov/brain/tumors/anatomy/neurons.html},
}

@misc{OpenAITokens,
  author       = {OpenAI},
  title        = {What are tokens and how to count them?},
  year         = {2024},
  howpublished = {\url{https://help.openai.com/en/articles/4936856-what-are-tokens-and-how-to-count-them}},
  note         = {Accessed: 2024-12-11},
}

@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, A},
  journal={Advances in Neural Information Processing Systems},
  year={2017}
}

@article{bahdanau2014neural,
  title={Neural machine translation by jointly learning to align and translate},
  author={Bahdanau, Dzmitry},
  journal={arXiv preprint arXiv:1409.0473},
  year={2014}
}

@incollection{torrey2010transfer,
  title={Transfer learning},
  author={Torrey, Lisa and Shavlik, Jude},
  booktitle={Handbook of research on machine learning applications and trends: algorithms, methods, and techniques},
  pages={242--264},
  year={2010},
  publisher={IGI global}
}

@misc{cross_lingual_transfer,
  title = {Cross-Lingual Transfer},
  howpublished = {\url{https://paperswithcode.com/task/cross-lingual-transfer}},
  note = {Accessed: 2024-12-17},
  year = {2024}
}

@article{pellegrini2006semantic,
  title={Semantic web},
  author={Pellegrini, Tassilo and Blumauer, Andreas},
  journal={Wege zur vernetzten Wissensgesellschaft. Berlin [ua] Springer},
  year={2006},
  publisher={Springer}
}

@misc{dataeuropa_rdf_sparql,
  author       = {{data.europa.eu}},
  title        = {Introduction to RDF \& SPARQL},
  year         = {2014},
  note         = {RDF was published as a W3C recommendation in 1999. RDF was originally introduced as a data model for metadata. RDF was generalised to cover knowledge of all kinds.},
  url          = {https://data.europa.eu/sites/default/files/d2.1.2_training_module_1.3_introduction_to_rdf_sparql_en_edp.pdf},
  howpublished = {Online}
}

@article{gupta2023continual,
  title={Continual Pre-Training of Large Language Models: How to (re) warm your model?},
  author={Gupta, Kshitij and Th{\'e}rien, Benjamin and Ibrahim, Adam and Richter, Mats L and Anthony, Quentin and Belilovsky, Eugene and Rish, Irina and Lesort, Timoth{\'e}e},
  journal={arXiv preprint arXiv:2308.04014},
  year={2023}
}

@article{bozinovski2020reminder,
  title={Reminder of the first paper on transfer learning in neural networks, 1976},
  author={Bozinovski, Stevo},
  journal={Informatica},
  volume={44},
  number={3},
  year={2020}
}

@article{HAN2021225,
title = {Pre-trained models: Past, present and future},
journal = {AI Open},
volume = {2},
pages = {225-250},
year = {2021},
issn = {2666-6510},
doi = {https://doi.org/10.1016/j.aiopen.2021.08.002},
url = {https://www.sciencedirect.com/science/article/pii/S2666651021000231},
author = {Xu Han and Zhengyan Zhang and Ning Ding and Yuxian Gu and Xiao Liu and Yuqi Huo and Jiezhong Qiu and Yuan Yao and Ao Zhang and Liang Zhang and Wentao Han and Minlie Huang and Qin Jin and Yanyan Lan and Yang Liu and Zhiyuan Liu and Zhiwu Lu and Xipeng Qiu and Ruihua Song and Jie Tang and Ji-Rong Wen and Jinhui Yuan and Wayne Xin Zhao and Jun Zhu},
keywords = {Pre-trained models, Language models, Transfer learning, Self-supervised learning, Natural language processing, Multimodal processing, Artificial intelligence},
abstract = {Large-scale pre-trained models (PTMs) such as BERT and GPT have recently achieved great success and become a milestone in the field of artificial intelligence (AI). Owing to sophisticated pre-training objectives and huge model parameters, large-scale PTMs can effectively capture knowledge from massive labeled and unlabeled data. By storing knowledge into huge parameters and fine-tuning on specific tasks, the rich knowledge implicitly encoded in huge parameters can benefit a variety of downstream tasks, which has been extensively demonstrated via experimental verification and empirical analysis. It is now the consensus of the AI community to adopt PTMs as backbone for downstream tasks rather than learning models from scratch. In this paper, we take a deep look into the history of pre-training, especially its special relation with transfer learning and self-supervised learning, to reveal the crucial position of PTMs in the AI development spectrum. Further, we comprehensively review the latest breakthroughs of PTMs. These breakthroughs are driven by the surge of computational power and the increasing availability of data, towards four important directions: designing effective architectures, utilizing rich contexts, improving computational efficiency, and conducting interpretation and theoretical analysis. Finally, we discuss a series of open problems and research directions of PTMs, and hope our view can inspire and advance the future study of PTMs.}
}

@article{WANG202351,
title = {Pre-Trained Language Models and Their Applications},
journal = {Engineering},
volume = {25},
pages = {51-65},
year = {2023},
issn = {2095-8099},
doi = {https://doi.org/10.1016/j.eng.2022.04.024},
url = {https://www.sciencedirect.com/science/article/pii/S2095809922006324},
author = {Haifeng Wang and Jiwei Li and Hua Wu and Eduard Hovy and Yu Sun},
keywords = {Pre-trained models, Natural language processing},
abstract = {Pre-trained language models have achieved striking success in natural language processing (NLP), leading to a paradigm shift from supervised learning to pre-training followed by fine-tuning. The NLP community has witnessed a surge of research interest in improving pre-trained models. This article presents a comprehensive review of representative work and recent progress in the NLP field and introduces the taxonomy of pre-trained models. We first give a brief introduction of pre-trained models, followed by characteristic methods and frameworks. We then introduce and analyze the impact and challenges of pre-trained models and their downstream applications. Finally, we briefly conclude and address future research directions in this field.}
}

@article{parthasarathy2024ultimate,
  title={The ultimate guide to fine-tuning llms from basics to breakthroughs: An exhaustive review of technologies, research, best practices, applied research challenges and opportunities},
  author={Parthasarathy, Venkatesh Balavadhani and Zafar, Ahtsham and Khan, Aafaq and Shahid, Arsalan},
  journal={arXiv preprint arXiv:2408.13296},
  year={2024}
}

@inproceedings{kim-etal-2017-cross,
    title = "Cross-Lingual Transfer Learning for {POS} Tagging without Cross-Lingual Resources",
    author = "Kim, Joo-Kyung  and
      Kim, Young-Bum  and
      Sarikaya, Ruhi  and
      Fosler-Lussier, Eric",
    editor = "Palmer, Martha  and
      Hwa, Rebecca  and
      Riedel, Sebastian",
    booktitle = "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing",
    month = sep,
    year = "2017",
    address = "Copenhagen, Denmark",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D17-1302",
    doi = "10.18653/v1/D17-1302",
    pages = "2832--2838",
    abstract = "Training a POS tagging model with crosslingual transfer learning usually requires linguistic knowledge and resources about the relation between the source language and the target language. In this paper, we introduce a cross-lingual transfer learning model for POS tagging without ancillary resources such as parallel corpora. The proposed cross-lingual model utilizes a common BLSTM that enables knowledge transfer from other languages, and private BLSTMs for language-specific representations. The cross-lingual model is trained with language-adversarial training and bidirectional language modeling as auxiliary objectives to better represent language-general information while not losing the information about a specific target language. Evaluating on POS datasets from 14 languages in the Universal Dependencies corpus, we show that the proposed transfer learning model improves the POS tagging performance of the target languages without exploiting any linguistic knowledge between the source language and the target language.",
}

@article{schuster2018cross,
  title={Cross-lingual transfer learning for multilingual task oriented dialog},
  author={Schuster, Sebastian and Gupta, Sonal and Shah, Rushin and Lewis, Mike},
  journal={arXiv preprint arXiv:1810.13327},
  year={2018}
}


@article{pfeiffer2020mad,
  title={Mad-x: An adapter-based framework for multi-task cross-lingual transfer},
  author={Pfeiffer, Jonas and Vuli{\'c}, Ivan and Gurevych, Iryna and Ruder, Sebastian},
  journal={arXiv preprint arXiv:2005.00052},
  year={2020}
}

@article{adelani2024comparing,
  title={Comparing LLM prompting with Cross-lingual transfer performance on Indigenous and Low-resource Brazilian Languages},
  author={Adelani, David Ifeoluwa and Do{\u{g}}ru{\"o}z, A Seza and Coneglian, Andr{\'e} and Ojha, Atul Kr},
  journal={arXiv preprint arXiv:2404.18286},
  year={2024}
}

@inproceedings{10.1145/3581783.3611992,
author = {Muraoka, Masayasu and Bhattacharjee, Bishwaranjan and Merler, Michele and Blackwood, Graeme and Li, Yulong and Zhao, Yang},
title = {Cross-Lingual Transfer of Large Language Model by Visually-Derived Supervision Toward Low-Resource Languages},
year = {2023},
isbn = {9798400701085},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3581783.3611992},
doi = {10.1145/3581783.3611992},
abstract = {Recent progress on vision and language research has shown that visual supervision improves the performance of large language models (LLMs) in various natural language processing (NLP) tasks. In particular, the Vokenization approach [65] initiated a new way of incorporating visual information into LLM training, demonstrating the potential of visual supervision for NLP tasks in a monolingual (i.e., English) setting. Given the effectiveness of visual information in human communication among people who speak different languages, we tackle an ambitious question in this paper; can we expect that visual supervision contributes to cross-lingual transfer learning from a high-resource language to low-resource languages in NLP tasks? To study this hypothesis, we build a cross-lingual Vokenization model and train a cross-lingual LLM on three languages, English, Urdu, and Swahili, in which the last two are considered low-resource languages. The experimental results demonstrate that our visually-supervised cross-lingual transfer learning method significantly improves the LLM performance in multiple cross-lingual NLP tasks such as XNLI, NER, and TyDiQA tasks for low-resource languages. We also qualitatively and quantitatively demonstrate that the benefit of our approach increases as the linguistic distance between low-and high-resource languages grows larger.},
booktitle = {Proceedings of the 31st ACM International Conference on Multimedia},
pages = {3637–3646},
numpages = {10},
keywords = {cross-lingual transfer, large language model training, low-resource languages, visual supervision},
location = {Ottawa ON, Canada},
series = {MM '23}
}

@article{wang2024probing,
  title={Probing the Emergence of Cross-lingual Alignment during LLM Training},
  author={Wang, Hetong and Minervini, Pasquale and Ponti, Edoardo M},
  journal={arXiv preprint arXiv:2406.13229},
  year={2024}
}

@article{fujinuma2022match,
  title={Match the script, adapt if multilingual: Analyzing the effect of multilingual pretraining on cross-lingual transferability},
  author={Fujinuma, Yoshinari and Boyd-Graber, Jordan and Kann, Katharina},
  journal={arXiv preprint arXiv:2203.10753},
  year={2022}
}

@inproceedings{efimov2023impact,
  title={The impact of cross-lingual adjustment of contextual word representations on zero-shot transfer},
  author={Efimov, Pavel and Boytsov, Leonid and Arslanova, Elena and Braslavski, Pavel},
  booktitle={European Conference on Information Retrieval},
  pages={51--67},
  year={2023},
  organization={Springer}
}

@article{fujinuma2022match,
  title={Match the script, adapt if multilingual: Analyzing the effect of multilingual pretraining on cross-lingual transferability},
  author={Fujinuma, Yoshinari and Boyd-Graber, Jordan and Kann, Katharina},
  journal={arXiv preprint arXiv:2203.10753},
  year={2022}
}

@inproceedings{avila2024experiments,
  title={Experiments with text-to-SPARQL based on ChatGPT},
  author={Avila, Caio Viktor S and Vidal, V{\^a}nia MP and Franco, Wellington and Casanova, Marco A},
  booktitle={2024 IEEE 18th International Conference on Semantic Computing (ICSC)},
  pages={277--284},
  year={2024},
  organization={IEEE}
}

@article{aws_prompt_engineering,
  title = {What is Prompt Engineering?},
  author = {{Amazon Web Services (AWS)}},
  year = {2025},
  url = {https://aws.amazon.com/what-is/prompt-engineering/},
  note = {Accessed: 2025-01-07},
  abstract = {Prompt engineering is the process where you guide generative artificial intelligence (generative AI) solutions to generate desired outputs. Even though generative AI attempts to mimic humans, it requires detailed instructions to create high-quality and relevant output. In prompt engineering, you choose the most appropriate formats, phrases, words, and symbols that guide the AI to interact with your users more meaningfully. Prompt engineers use creativity plus trial and error to create a collection of input texts, sofan application's generative AI works as expected.},
}

@misc{zahera2024generating,
  title={Generating sparql from natural language using chain-of-thoughts prompting},
  author={Zahera, Hamada M and Ali, Manzoor and Sherif, Mohamed Ahmed and Moussallem, Diego and Ngomo, A-C Ngonga},
  year={2024},
  publisher={SEMANTiCS}
}

@article{emonet2024llm,
  title={LLM-based SPARQL Query Generation from Natural Language over Federated Knowledge Graphs},
  author={Emonet, Vincent and Bolleman, Jerven and Duvaud, Severine and de Farias, Tarcisio Mendes and Sima, Ana Claudia},
  journal={arXiv preprint arXiv:2410.06062},
  year={2024}
}

@article{LOPEZ20133,
title = {Evaluating question answering over linked data},
journal = {Journal of Web Semantics},
volume = {21},
pages = {3-13},
year = {2013},
note = {Special Issue on Evaluation of Semantic Technologies},
issn = {1570-8268},
doi = {https://doi.org/10.1016/j.websem.2013.05.006},
url = {https://www.sciencedirect.com/science/article/pii/S157082681300022X},
author = {Vanessa Lopez and Christina Unger and Philipp Cimiano and Enrico Motta},
keywords = {Evaluation, Question answering, Semantic Web, Linked data, Natural language},
abstract = {The availability of large amounts of open, distributed, and structured semantic data on the web has no precedent in the history of computer science. In recent years, there have been important advances in semantic search and question answering over RDF data. In particular, natural language interfaces to online semantic data have the advantage that they can exploit the expressive power of Semantic Web data models and query languages, while at the same time hiding their complexity from the user. However, despite the increasing interest in this area, there are no evaluations so far that systematically evaluate this kind of systems, in contrast to traditional question answering and search interfaces to document spaces. To address this gap, we have set up a series of evaluation challenges for question answering over linked data. The main goal of the challenge was to get insight into the strengths, capabilities, and current shortcomings of question answering systems as interfaces to query linked data sources, as well as benchmarking how these interaction paradigms can deal with the fact that the amount of RDF data available on the web is very large and heterogeneous with respect to the vocabularies and schemas used. Here, we report on the results from the first and second of such evaluation campaigns. We also discuss how the second evaluation addressed some of the issues and limitations which arose from the first one, as well as the open issues to be addressed in future competitions.}
}

@article{jiang2022knowledge,
  title={Knowledge Graph Question Answering Datasets and Their Generalizability: Are They Enough for Future Research?},
  author={Jiang, Longquan and Usbeck, Ricardo},
  journal={arXiv preprint arXiv:2205.06573},
  year={2022}
}


@inproceedings{Talmor2018TheWA,
            title={The Web as a Knowledge-Base for Answering Complex Questions},
            author={Alon Talmor and Jonathan Berant},
            booktitle={NAACL},
            year={2018}
}

@inproceedings{saxena2021cronkgqa,
      title={Question Answering over Temporal Knowledge Graphs},
      author={Saxena, Apoorv and Chakrabarti, Soumen and Talukdar, Partha},
      booktitle={Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics},
      year={2021}
    }

@ARTICLE{2021arXiv210803509C,
        author = {{Cui}, Ruixiang and {Aralikatte}, Rahul and {Lent}, Heather and {Hershcovich}, Daniel},
        title = "{Multilingual Compositional Wikidata Questions}",
        journal = {arXiv e-prints},
        keywords = {Computer Science - Computation and Language},
        year = 2021,
        month = aug,
        eid = {arXiv:2108.03509},
        pages = {arXiv:2108.03509},
        archivePrefix = {arXiv},
        eprint = {2108.03509},
        primaryClass = {cs.CL},
        adsurl = {https://ui.adsabs.harvard.edu/abs/2021arXiv210803509C},
        adsnote = {Provided by the SAO/NASA Astrophysics Data System}
    }

@inproceedings{gu2021beyond,
            title={Beyond IID: three levels of generalization for question answering on knowledge bases},
            author={Gu, Yu and Kase, Sue and Vanni, Michelle and Sadler, Brian and Liang, Percy and Yan, Xifeng and Su, Yu},
            booktitle={Proceedings of the Web Conference 2021},
            pages={3477--3488},
            organization={ACM}
        }

@inproceedings{su-etal-2016-generating,
            title = "On Generating Characteristic-rich Question Sets for {QA} Evaluation",
            author = {Su, Yu and Sun, Huan and Sadler, Brian and Srivatsa, Mudhakar and G{\"u}r, Izzeddin and Yan, Zenghui and Yan, Xifeng},
            booktitle = "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing",
            month = nov,
            year = "2016",
            address = "Austin, Texas",
            publisher = "Association for Computational Linguistics",
            url = "https://www.aclweb.org/anthology/D16-1054",
            doi = "10.18653/v1/D16-1054",
            pages = "562--572",
        }

@inproceedings{trivedi2017lc,
        title={Lc-quad: A corpus for complex question answering over knowledge graphs},
        author={Trivedi, Priyansh and Maheshwari, Gaurav and Dubey, Mohnish and Lehmann, Jens},
        booktitle={International Semantic Web Conference},
        pages={210--218},
        year={2017},
        organization={Springer}
    }

@inproceedings{dubey2017lc2,
        title={LC-QuAD 2.0: A Large Dataset for Complex Question Answering over Wikidata and DBpedia},
        author={Dubey, Mohnish and Banerjee, Debayan and Abdelkawi, Abdelrahman and Lehmann, Jens},
        booktitle={Proceedings of the 18th International Semantic Web Conference (ISWC)},
        year={2019},
        organization={Springer}
    }

@inproceedings{DBLP:conf/kcap/KaffeeESV19,
    title     = {Ranking Knowledge Graphs By Capturing Knowledge about Languages and
               Labels},
    author    = {Lucie{-}Aim{\'{e}}e Kaffee and
               Kemele M. Endris and
               Elena Simperl and
               Maria{-}Esther Vidal},
    booktitle = {Proceedings of the 10th International Conference on Knowledge Capture,
               {K-CAP} 2019, Marina Del Rey, CA, USA, November 19-21, 2019},
    publisher = {{ACM}},
    year      = {2019},
    url       = {https://doi.org/10.1145/3360901.3364443}
}
 @ARTICLE{2020arXiv200510659K,
        author = {{Korablinov}, Vladislav and {Braslavski}, Pavel},
        title = "{RuBQ: A Russian Dataset for Question Answering over Wikidata}",
        journal = {arXiv e-prints},
        keywords = {Computer Science - Computation and Language},
        year = 2020,
        month = may,
        eid = {arXiv:2005.10659},
        pages = {arXiv:2005.10659},
        archivePrefix = {arXiv},
        eprint = {2005.10659},
        primaryClass = {cs.CL},
        adsurl = {https://ui.adsabs.harvard.edu/abs/2020arXiv200510659K},
        adsnote = {Provided by the SAO/NASA Astrophysics Data System}
    }

@inproceedings{
        rybin2021rubq,
        title={Ru{\{}BQ{\}} 2.0: An Innovated Russian Question Answering Dataset},
        author={Ivan Rybin and Vladislav Korablinov and Pavel Efimov and Pavel Braslavski},
        booktitle={Eighteenth Extended Semantic Web Conference - Resources Track},
        year={2021},
        url={https://openreview.net/forum?id=P5UQFFoQ4PJ}
    }

@inproceedings{yih-etal-2016-value,
        title={The Value of Semantic Parse Labeling for Knowledge Base Question Answering},
        author={Yih, Wen-tau and Richardson, Matthew and Meek, Chris and Chang, Ming-Wei and Suh, Jina},
        booktitle={Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
        year={2016},
        publisher={Association for Computational Linguistics},
        pages={201--206},
        }

@misc{MITx6.036,
  author       = "{Massachusetts Institute of Technology}",
  title        = "{Introduction to Machine Learning (6.036)}",
  year         = {2020},
  howpublished = "\url{https://openlearninglibrary.mit.edu/courses/course-v1:MITx+6.036+1T2019/course/}",
  note         = "Accessed: 2025-03-08"}
}

@article{lipton2014thresholding,
  title={Thresholding classifiers to maximize F1 score},
  author={Lipton, Zachary Chase and Elkan, Charles and Narayanaswamy, Balakrishnan},
  journal={arXiv preprint arXiv:1402.1892},
  year={2014}
}

@article{ghassemiazghandi2024evaluation,
  title={An Evaluation of ChatGPT's Translation Accuracy Using BLEU Score},
  author={Ghassemiazghandi, Mozhgan},
  journal={Theory and Practice in Language Studies},
  volume={14},
  number={4},
  pages={985--994},
  year={2024},
  publisher={Academy Publication Co., Ltd.}
}

@article{osisanwo2017supervised,
  title={Supervised machine learning algorithms: classification and comparison},
  author={Osisanwo, FY and Akinsola, JET and Awodele, O and Hinmikaiye, JO and Olakanmi, O and Akinjobi, J and others},
  journal={International Journal of Computer Trends and Technology (IJCTT)},
  volume={48},
  number={3},
  pages={128--138},
  year={2017}
}

@article{webb2010naive,
  title={Na{\"\i}ve Bayes.},
  author={Webb, Geoffrey I and Keogh, Eamonn and Miikkulainen, Risto},
  journal={Encyclopedia of machine learning},
  volume={15},
  number={1},
  pages={713--714},
  year={2010}
}

@book{goodfellow2016deep,
  title={Deep learning},
  author={Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron and Bengio, Yoshua},
  volume={1},
  number={2},
  year={2016},
  publisher={MIT press Cambridge}
}